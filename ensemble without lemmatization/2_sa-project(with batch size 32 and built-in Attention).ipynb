{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10223141,"sourceType":"datasetVersion","datasetId":6319953},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n    text = re.sub(r\"@\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# Function to preprocess text\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")\n\n# Encode sentiment labels\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + MultiHeadAttention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3, num_heads=8):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # MultiHeadAttention layers\n        self.lstm_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=lstm_units)\n        self.bilstm_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=lstm_units * 2)\n        self.gru_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=lstm_units)\n        \n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply MultiHeadAttention to each recurrent layer\n        lstm_attn = self.lstm_attn(lstm_out, lstm_out)\n        bilstm_attn = self.bilstm_attn(bilstm_out, bilstm_out)\n        gru_attn = self.gru_attn(gru_out, gru_out)\n\n        # Merge outputs\n        merged = self.concat([tf.reduce_mean(lstm_attn, axis=1),\n                              tf.reduce_mean(bilstm_attn, axis=1),\n                              tf.reduce_mean(gru_attn, axis=1)])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=50, batch_size=32, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:27:20.350270Z","iopub.execute_input":"2025-03-24T11:27:20.350607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9854c7d51bf4ded9fa2bce38abb6277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd645c690804486a9bc9f2dfd71a604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66aac993f06c4c7996d9a270f6fbedc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5be9a31f22884cc6ae14e2bed1572147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1a32ed85f745308dae434b63ef7899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be7029cf76d403c9c6282f8b1f30436"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n366/366 [==============================] - 252s 578ms/step - loss: 0.4942 - accuracy: 0.8051 - val_loss: 0.3904 - val_accuracy: 0.8535\nEpoch 2/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.3460 - accuracy: 0.8684 - val_loss: 0.3935 - val_accuracy: 0.8538\nEpoch 3/50\n366/366 [==============================] - 206s 564ms/step - loss: 0.2549 - accuracy: 0.9074 - val_loss: 0.4251 - val_accuracy: 0.8552\nEpoch 4/50\n366/366 [==============================] - 206s 564ms/step - loss: 0.1843 - accuracy: 0.9360 - val_loss: 0.4474 - val_accuracy: 0.8501\nEpoch 5/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.1400 - accuracy: 0.9510 - val_loss: 0.5707 - val_accuracy: 0.8473\nEpoch 6/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.1035 - accuracy: 0.9661 - val_loss: 0.6545 - val_accuracy: 0.8494\nEpoch 7/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0850 - accuracy: 0.9716 - val_loss: 0.6345 - val_accuracy: 0.8545\nEpoch 8/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0744 - accuracy: 0.9765 - val_loss: 0.6122 - val_accuracy: 0.8443\nEpoch 9/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0677 - accuracy: 0.9778 - val_loss: 0.6959 - val_accuracy: 0.8501\nEpoch 10/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0581 - accuracy: 0.9813 - val_loss: 0.6782 - val_accuracy: 0.8508\nEpoch 11/50\n366/366 [==============================] - 207s 566ms/step - loss: 0.0444 - accuracy: 0.9855 - val_loss: 0.7141 - val_accuracy: 0.8422\nEpoch 12/50\n366/366 [==============================] - 207s 566ms/step - loss: 0.0475 - accuracy: 0.9837 - val_loss: 0.7505 - val_accuracy: 0.8385\nEpoch 13/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0403 - accuracy: 0.9869 - val_loss: 0.8055 - val_accuracy: 0.8484\nEpoch 14/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0372 - accuracy: 0.9877 - val_loss: 0.8533 - val_accuracy: 0.8508\nEpoch 15/50\n366/366 [==============================] - 207s 564ms/step - loss: 0.0383 - accuracy: 0.9871 - val_loss: 0.7742 - val_accuracy: 0.8446\nEpoch 16/50\n366/366 [==============================] - 207s 564ms/step - loss: 0.0418 - accuracy: 0.9864 - val_loss: 0.7531 - val_accuracy: 0.8408\nEpoch 17/50\n366/366 [==============================] - 207s 564ms/step - loss: 0.0400 - accuracy: 0.9859 - val_loss: 0.7831 - val_accuracy: 0.8487\nEpoch 18/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 0.7763 - val_accuracy: 0.8453\nEpoch 19/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 0.7887 - val_accuracy: 0.8511\nEpoch 20/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0282 - accuracy: 0.9890 - val_loss: 0.9234 - val_accuracy: 0.8477\nEpoch 21/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0304 - accuracy: 0.9909 - val_loss: 0.8324 - val_accuracy: 0.8467\nEpoch 22/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0287 - accuracy: 0.9912 - val_loss: 0.8340 - val_accuracy: 0.8490\nEpoch 23/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0289 - accuracy: 0.9902 - val_loss: 0.9423 - val_accuracy: 0.8518\nEpoch 24/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.8339 - val_accuracy: 0.8371\nEpoch 25/50\n366/366 [==============================] - 207s 565ms/step - loss: 0.0244 - accuracy: 0.9913 - val_loss: 0.9086 - val_accuracy: 0.8453\nEpoch 26/50\n236/366 [==================>...........] - ETA: 1:07 - loss: 0.0231 - accuracy: 0.9919","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n    text = re.sub(r\"@\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# Function to preprocess text\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-50k-movies/IMDb_50K_movies/IMDB Dataset.csv\")\n\n# Encode sentiment labels\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + MultiHeadAttention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3, num_heads=8):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # MultiHeadAttention layers\n        self.lstm_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=lstm_units)\n        self.bilstm_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=lstm_units * 2)\n        self.gru_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=lstm_units)\n        \n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply MultiHeadAttention to each recurrent layer\n        lstm_attn = self.lstm_attn(lstm_out, lstm_out)\n        bilstm_attn = self.bilstm_attn(bilstm_out, bilstm_out)\n        gru_attn = self.gru_attn(gru_out, gru_out)\n\n        # Merge outputs\n        merged = self.concat([tf.reduce_mean(lstm_attn, axis=1),\n                              tf.reduce_mean(bilstm_attn, axis=1),\n                              tf.reduce_mean(gru_attn, axis=1)])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=50, batch_size=32, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:31:24.273984Z","iopub.execute_input":"2025-03-24T13:31:24.274265Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"352ac4fa7e9d46a682cba24698ad29d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"488b57d36a924f08af0d7cb42ea17931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e22ef25f321449e5b97f8b210f552516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28aa83e5c414bc8b8a2c4e5cb4d3325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caabff7a3ec04c13b3d58f086bd364f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6654f2099de4835a0f150d4f9b870de"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n1250/1250 [==============================] - 752s 571ms/step - loss: 0.2758 - accuracy: 0.8818 - val_loss: 0.2207 - val_accuracy: 0.9120\nEpoch 2/50\n1250/1250 [==============================] - 707s 566ms/step - loss: 0.1895 - accuracy: 0.9244 - val_loss: 0.2239 - val_accuracy: 0.9159\nEpoch 3/50\n1032/1250 [=======================>......] - ETA: 1:53 - loss: 0.1323 - accuracy: 0.9504","output_type":"stream"}],"execution_count":null}]}