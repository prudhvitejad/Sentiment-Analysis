{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10223141,"sourceType":"datasetVersion","datasetId":6319953},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    return text\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=50, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:56:57.747471Z","iopub.execute_input":"2025-03-14T18:56:57.747806Z","execution_failed":"2025-03-14T22:54:25.291Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602c82ed51dd4f148f7284c68b152460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfba6431f2a44494880e81172d5860a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76d03cb542c4192962b2c90844df520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54784b1f64a34496907d5dfbf2263d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afd143e853b45c194b50d5a8b461ba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09d9fcdc81e34e898402e57600ceba23"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n1464/1464 [==============================] - 303s 177ms/step - loss: 0.4867 - accuracy: 0.8089 - val_loss: 0.4543 - val_accuracy: 0.8186\nEpoch 2/50\n1464/1464 [==============================] - 254s 174ms/step - loss: 0.3523 - accuracy: 0.8671 - val_loss: 0.4016 - val_accuracy: 0.8504\nEpoch 3/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.2623 - accuracy: 0.9045 - val_loss: 0.4381 - val_accuracy: 0.8487\nEpoch 4/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.1902 - accuracy: 0.9299 - val_loss: 0.5407 - val_accuracy: 0.8487\nEpoch 5/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.1370 - accuracy: 0.9528 - val_loss: 0.5880 - val_accuracy: 0.8374\nEpoch 6/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.1122 - accuracy: 0.9606 - val_loss: 0.6115 - val_accuracy: 0.8354\nEpoch 7/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0926 - accuracy: 0.9678 - val_loss: 0.6738 - val_accuracy: 0.8323\nEpoch 8/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0819 - accuracy: 0.9734 - val_loss: 0.8746 - val_accuracy: 0.8484\nEpoch 9/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0713 - accuracy: 0.9769 - val_loss: 0.6546 - val_accuracy: 0.8473\nEpoch 10/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0660 - accuracy: 0.9784 - val_loss: 0.8354 - val_accuracy: 0.8367\nEpoch 11/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0628 - accuracy: 0.9797 - val_loss: 0.8044 - val_accuracy: 0.8258\nEpoch 12/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0585 - accuracy: 0.9816 - val_loss: 0.6831 - val_accuracy: 0.8340\nEpoch 13/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0563 - accuracy: 0.9812 - val_loss: 0.8991 - val_accuracy: 0.8265\nEpoch 14/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0472 - accuracy: 0.9843 - val_loss: 0.7553 - val_accuracy: 0.8385\nEpoch 15/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0500 - accuracy: 0.9838 - val_loss: 0.9114 - val_accuracy: 0.8374\nEpoch 16/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0450 - accuracy: 0.9857 - val_loss: 0.8945 - val_accuracy: 0.8344\nEpoch 17/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0437 - accuracy: 0.9862 - val_loss: 0.8563 - val_accuracy: 0.8460\nEpoch 18/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0430 - accuracy: 0.9865 - val_loss: 0.9543 - val_accuracy: 0.8415\nEpoch 19/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0372 - accuracy: 0.9879 - val_loss: 0.8443 - val_accuracy: 0.8374\nEpoch 20/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0399 - accuracy: 0.9866 - val_loss: 0.9012 - val_accuracy: 0.8374\nEpoch 21/50\n1464/1464 [==============================] - 254s 173ms/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.9316 - val_accuracy: 0.8449\nEpoch 22/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.8605 - val_accuracy: 0.8385\nEpoch 23/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0355 - accuracy: 0.9880 - val_loss: 0.7998 - val_accuracy: 0.8405\nEpoch 24/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.8709 - val_accuracy: 0.8422\nEpoch 25/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0282 - accuracy: 0.9897 - val_loss: 1.1269 - val_accuracy: 0.8292\nEpoch 26/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0338 - accuracy: 0.9886 - val_loss: 1.0172 - val_accuracy: 0.8374\nEpoch 27/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0371 - accuracy: 0.9869 - val_loss: 1.0429 - val_accuracy: 0.8371\nEpoch 28/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0264 - accuracy: 0.9906 - val_loss: 0.9846 - val_accuracy: 0.8350\nEpoch 29/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.9875 - val_accuracy: 0.8347\nEpoch 30/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0321 - accuracy: 0.9890 - val_loss: 0.9412 - val_accuracy: 0.8361\nEpoch 31/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0317 - accuracy: 0.9893 - val_loss: 0.9785 - val_accuracy: 0.8415\nEpoch 32/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0293 - accuracy: 0.9907 - val_loss: 0.9762 - val_accuracy: 0.8330\nEpoch 33/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0291 - accuracy: 0.9906 - val_loss: 0.9986 - val_accuracy: 0.8426\nEpoch 34/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0342 - accuracy: 0.9876 - val_loss: 1.0378 - val_accuracy: 0.8289\nEpoch 35/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.9607 - val_accuracy: 0.8357\nEpoch 36/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0322 - accuracy: 0.9888 - val_loss: 1.1321 - val_accuracy: 0.8408\nEpoch 37/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0267 - accuracy: 0.9910 - val_loss: 1.0478 - val_accuracy: 0.8364\nEpoch 38/50\n1464/1464 [==============================] - 251s 172ms/step - loss: 0.0278 - accuracy: 0.9903 - val_loss: 1.0730 - val_accuracy: 0.8323\nEpoch 39/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0278 - accuracy: 0.9901 - val_loss: 1.1041 - val_accuracy: 0.8398\nEpoch 40/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0279 - accuracy: 0.9901 - val_loss: 1.0597 - val_accuracy: 0.8344\nEpoch 41/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0245 - accuracy: 0.9921 - val_loss: 1.0184 - val_accuracy: 0.8333\nEpoch 42/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 1.3651 - val_accuracy: 0.8309\nEpoch 43/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0243 - accuracy: 0.9919 - val_loss: 1.0746 - val_accuracy: 0.8234\nEpoch 44/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0300 - accuracy: 0.9910 - val_loss: 1.1744 - val_accuracy: 0.8361\nEpoch 45/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 1.0366 - val_accuracy: 0.8415\nEpoch 46/50\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.0249 - accuracy: 0.9909 - val_loss: 0.7907 - val_accuracy: 0.8378\nEpoch 47/50\n 909/1464 [=================>............] - ETA: 1:27 - loss: 0.0198 - accuracy: 0.9930","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    return text\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-50k-movies/IMDb_50K_movies/IMDB Dataset.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=50, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T06:20:00.252902Z","iopub.execute_input":"2025-03-15T06:20:00.253256Z","execution_failed":"2025-03-15T15:06:55.818Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a442329d98847c898637564c7628fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74f78c7b7584ef6a8bcdbfed606015e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397e81a3067a4bf68baccfa4bf574728"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4b8511c4f64cf5a6bc560b5db81802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d61b0a725143589342a1297c131221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331fef832fa840158f4677bf6c0fb25a"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n5000/5000 [==============================] - 901s 172ms/step - loss: 0.2905 - accuracy: 0.8765 - val_loss: 0.2256 - val_accuracy: 0.9085\nEpoch 2/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.2025 - accuracy: 0.9193 - val_loss: 0.2344 - val_accuracy: 0.9073\nEpoch 3/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.1473 - accuracy: 0.9434 - val_loss: 0.2580 - val_accuracy: 0.9099\nEpoch 4/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.1085 - accuracy: 0.9611 - val_loss: 0.2799 - val_accuracy: 0.9094\nEpoch 5/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0827 - accuracy: 0.9700 - val_loss: 0.3300 - val_accuracy: 0.9090\nEpoch 6/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0640 - accuracy: 0.9780 - val_loss: 0.3335 - val_accuracy: 0.9002\nEpoch 7/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.4032 - val_accuracy: 0.9058\nEpoch 8/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0459 - accuracy: 0.9840 - val_loss: 0.4290 - val_accuracy: 0.9058\nEpoch 9/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0410 - accuracy: 0.9858 - val_loss: 0.4525 - val_accuracy: 0.9054\nEpoch 10/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0371 - accuracy: 0.9874 - val_loss: 0.4338 - val_accuracy: 0.9037\nEpoch 11/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.3732 - val_accuracy: 0.9013\nEpoch 12/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 0.4419 - val_accuracy: 0.8979\nEpoch 13/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0305 - accuracy: 0.9896 - val_loss: 0.4609 - val_accuracy: 0.9046\nEpoch 14/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0271 - accuracy: 0.9905 - val_loss: 0.3725 - val_accuracy: 0.9024\nEpoch 15/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0256 - accuracy: 0.9919 - val_loss: 0.5108 - val_accuracy: 0.9074\nEpoch 16/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0258 - accuracy: 0.9916 - val_loss: 0.4713 - val_accuracy: 0.8994\nEpoch 17/50\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.0253 - accuracy: 0.9912 - val_loss: 0.4286 - val_accuracy: 0.9000\nEpoch 18/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 0.5395 - val_accuracy: 0.9001\nEpoch 19/50\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.5140 - val_accuracy: 0.9064\nEpoch 20/50\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.0215 - accuracy: 0.9929 - val_loss: 0.4648 - val_accuracy: 0.9027\nEpoch 21/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.4581 - val_accuracy: 0.8998\nEpoch 22/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0206 - accuracy: 0.9930 - val_loss: 0.4941 - val_accuracy: 0.9099\nEpoch 23/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0186 - accuracy: 0.9934 - val_loss: 0.6157 - val_accuracy: 0.9040\nEpoch 24/50\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.5299 - val_accuracy: 0.9061\nEpoch 25/50\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.0187 - accuracy: 0.9935 - val_loss: 0.5718 - val_accuracy: 0.9020\nEpoch 26/50\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.5630 - val_accuracy: 0.9030\nEpoch 27/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.4717 - val_accuracy: 0.9051\nEpoch 28/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.5877 - val_accuracy: 0.8985\nEpoch 29/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 0.5327 - val_accuracy: 0.9013\nEpoch 30/50\n3426/5000 [===================>..........] - ETA: 4:08 - loss: 0.0149 - accuracy: 0.9952","output_type":"stream"}],"execution_count":null}]}