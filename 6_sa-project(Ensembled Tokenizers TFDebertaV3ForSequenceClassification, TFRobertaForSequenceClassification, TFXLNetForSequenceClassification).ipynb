{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import (DebertaV2Tokenizer, TFDebertaV3ForSequenceClassification,\n                          RobertaTokenizer, TFRobertaForSequenceClassification,\n                          XLNetTokenizer, TFXLNetForSequenceClassification)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Load Dataset\nfile_path = \"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\"\ndf = pd.read_csv(file_path)\n\n# Preprocess Text\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n    text = re.sub(r\"@\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text.strip()\n\ndf[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n\n# Encode Labels\nsentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\ndf[\"label\"] = df[\"airline_sentiment\"].map(sentiment_mapping)\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"label\"], test_size=0.2, random_state=42)\n\n# Hyperparameters\nMAX_LENGTH = 128\nBATCH_SIZE = 32\nEPOCHS = 5\nLEARNING_RATE = 3e-5\nNUM_CLASSES = 3  # Change this according to your task\n\n# Callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n\n# ------------------------- 1. TFDeBERTa Model -------------------------\ndeberta_name = \"microsoft/deberta-v3-base\"\ndeberta_tokenizer = DebertaV2Tokenizer.from_pretrained(deberta_name)\ndeberta_model = TFDebertaV3ForSequenceClassification.from_pretrained(deberta_name, num_labels=NUM_CLASSES)\n\n# Tokenize Data for DeBERTa\ndeberta_train_encodings = deberta_tokenizer(list(X_train), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\ndeberta_test_encodings = deberta_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n\n# TensorFlow Dataset for DeBERTa\ndeberta_train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(deberta_train_encodings),\n    y_train\n)).shuffle(len(X_train)).batch(BATCH_SIZE)\n\ndeberta_test_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(deberta_test_encodings),\n    y_test\n)).batch(BATCH_SIZE)\n\n# Compile and Train DeBERTa Model\ndeberta_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=deberta_model.compute_loss, metrics=['accuracy'])\ndeberta_model.fit(deberta_train_dataset, validation_data=deberta_test_dataset, epochs=EPOCHS, callbacks=[early_stopping, reduce_lr])\n\n# ------------------------- 2. TFRoBERTa Model -------------------------\nroberta_name = \"roberta-base\"\nroberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_name)\nroberta_model = TFRobertaForSequenceClassification.from_pretrained(roberta_name, num_labels=NUM_CLASSES)\n\n# Tokenize Data for RoBERTa\nroberta_train_encodings = roberta_tokenizer(list(X_train), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\nroberta_test_encodings = roberta_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n\n# TensorFlow Dataset for RoBERTa\nroberta_train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(roberta_train_encodings),\n    y_train\n)).shuffle(len(X_train)).batch(BATCH_SIZE)\n\nroberta_test_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(roberta_test_encodings),\n    y_test\n)).batch(BATCH_SIZE)\n\n# Compile and Train RoBERTa Model\nroberta_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=roberta_model.compute_loss, metrics=['accuracy'])\nroberta_model.fit(roberta_train_dataset, validation_data=roberta_test_dataset, epochs=EPOCHS, callbacks=[early_stopping, reduce_lr])\n\n# ------------------------- 3. TFXLNet Model -------------------------\nxlnet_name = \"xlnet-base-cased\"\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_name)\nxlnet_model = TFXLNetForSequenceClassification.from_pretrained(xlnet_name, num_labels=NUM_CLASSES)\n\n# Tokenize Data for XLNet\nxlnet_train_encodings = xlnet_tokenizer(list(X_train), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\nxlnet_test_encodings = xlnet_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n\n# TensorFlow Dataset for XLNet\nxlnet_train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(xlnet_train_encodings),\n    y_train\n)).shuffle(len(X_train)).batch(BATCH_SIZE)\n\nxlnet_test_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(xlnet_test_encodings),\n    y_test\n)).batch(BATCH_SIZE)\n\n# Compile and Train XLNet Model\nxlnet_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=xlnet_model.compute_loss, metrics=['accuracy'])\nxlnet_model.fit(xlnet_train_dataset, validation_data=xlnet_test_dataset, epochs=EPOCHS, callbacks=[early_stopping, reduce_lr])\n\n# ------------------------- 4. Ensemble Predictions -------------------------\ndef get_ensemble_predictions(models, test_dataset):\n    predictions = []\n    for model in models:\n        preds = model.predict(test_dataset).logits\n        probs = tf.nn.softmax(preds, axis=1).numpy()\n        predictions.append(probs)\n    \n    # Average the probabilities\n    avg_probs = np.mean(predictions, axis=0)\n    ensemble_preds = np.argmax(avg_probs, axis=1)\n    return ensemble_preds\n\n# Get predictions from all models\nmodels = [deberta_model, roberta_model, xlnet_model]\nensemble_preds = get_ensemble_predictions(models, deberta_test_dataset)\n\n# Evaluate Accuracy\naccuracy = accuracy_score(y_test, ensemble_preds)\nprint(f'Ensemble Test Accuracy: {accuracy:.4f}')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:07:20.704290Z","iopub.execute_input":"2025-02-13T14:07:20.704670Z","iopub.status.idle":"2025-02-13T14:07:28.377206Z","shell.execute_reply.started":"2025-02-13T14:07:20.704638Z","shell.execute_reply":"2025-02-13T14:07:28.376002Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a4868204daef>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from transformers import (DebertaV2Tokenizer, TFDebertaV3ForSequenceClassification,\n\u001b[0m\u001b[1;32m      9\u001b[0m                           \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFRobertaForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                           XLNetTokenizer, TFXLNetForSequenceClassification)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'TFDebertaV3ForSequenceClassification' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'TFDebertaV3ForSequenceClassification' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)","output_type":"error"}],"execution_count":9}]}