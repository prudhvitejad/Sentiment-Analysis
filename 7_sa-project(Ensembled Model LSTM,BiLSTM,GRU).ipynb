{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10223141,"sourceType":"datasetVersion","datasetId":6319953},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    return text\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=3, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T06:07:01.616601Z","iopub.execute_input":"2025-03-11T06:07:01.616954Z","iopub.status.idle":"2025-03-11T06:23:29.375629Z","shell.execute_reply.started":"2025-03-11T06:07:01.616925Z","shell.execute_reply":"2025-03-11T06:23:29.374932Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b51427040544ef99303e203831dd1cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78bb21f78bf441d904ef129362126ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10c7ec3d51c44029b09f63366b638ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5595a4721a9e43b38e2149bbd1168deb"}},"metadata":{}},{"name":"stderr","text":"Error while downloading from https://huggingface.co/roberta-base/resolve/main/tokenizer.json: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\nTrying to resume download...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d78f27d9964457f81187505ae8b255e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c7f85a610a41d48d85fdd28444b3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56319d2b7534206a6ec4c9caf98326f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n1464/1464 [==============================] - 293s 173ms/step - loss: 0.4898 - accuracy: 0.8087 - val_loss: 0.4011 - val_accuracy: 0.8494\nEpoch 2/3\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.3363 - accuracy: 0.8746 - val_loss: 0.4192 - val_accuracy: 0.8429\nEpoch 3/3\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.2555 - accuracy: 0.9083 - val_loss: 0.4934 - val_accuracy: 0.8460\n92/92 [==============================] - 21s 182ms/step - loss: 0.4934 - accuracy: 0.8460\nTest Accuracy: 0.8460\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    return text\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-50k-movies/IMDb_50K_movies/IMDB Dataset.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=3, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T12:37:05.190975Z","iopub.execute_input":"2025-03-11T12:37:05.191279Z","iopub.status.idle":"2025-03-11T13:23:43.913687Z","shell.execute_reply.started":"2025-03-11T12:37:05.191251Z","shell.execute_reply":"2025-03-11T13:23:43.912844Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6928f93f45148218efd15b8efdaeb5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83f257724974bdebea6bc1d07e245da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e067742a2e1a47e4b895fa4a9df20c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7902a7ab4648bab710b649f86782cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9ffcfa92d540a6b81fb1ecb6b52ed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04425209b43f496182f5d4aea50be422"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n5000/5000 [==============================] - 898s 172ms/step - loss: 0.2914 - accuracy: 0.8767 - val_loss: 0.2806 - val_accuracy: 0.8839\nEpoch 2/3\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.2038 - accuracy: 0.9193 - val_loss: 0.2293 - val_accuracy: 0.9085\nEpoch 3/3\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.1457 - accuracy: 0.9458 - val_loss: 0.2821 - val_accuracy: 0.9056\n313/313 [==============================] - 62s 183ms/step - loss: 0.2821 - accuracy: 0.9056\nTest Accuracy: 0.9056\n","output_type":"stream"}],"execution_count":1}]}