{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModelForSequenceClassification, AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\n\n# Load dataset\nfile_path = \"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\"\ndf = pd.read_csv(file_path)\n\n# Preprocess text\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n    text = re.sub(r\"@\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text.strip()\n\ndf[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n\n# Encode labels\nsentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\ndf[\"label\"] = df[\"airline_sentiment\"].map(sentiment_mapping)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"label\"], test_size=0.2, random_state=42)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\n# Tokenization\ndef encode_texts(texts, tokenizer, max_len=128):\n    return tokenizer(list(texts), max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n\nX_train_enc = encode_texts(X_train, tokenizer)\nX_test_enc = encode_texts(X_test, tokenizer)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train_enc), y_train)).shuffle(1000).batch(32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_enc), y_test)).batch(32)\n\n# Load RoBERTa model\nroberta_model = TFAutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3, from_pt=True)\n\n# Define optimizer and loss\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Define accuracy metric\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Custom Training Step\n@tf.function\ndef train_step(batch_inputs, batch_labels):\n    with tf.GradientTape() as tape:\n        logits = roberta_model(batch_inputs, training=True).logits\n        loss = loss_fn(batch_labels, logits)\n    \n    gradients = tape.gradient(loss, roberta_model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, roberta_model.trainable_variables))\n\n    # Update accuracy metric\n    train_accuracy.update_state(batch_labels, logits)\n    \n    return loss\n\n# Custom Testing Step\n@tf.function\ndef test_step(batch_inputs, batch_labels):\n    logits = roberta_model(batch_inputs, training=False).logits\n    loss = loss_fn(batch_labels, logits)\n    \n    # Update accuracy metric\n    test_accuracy.update_state(batch_labels, logits)\n    \n    return loss\n\n# Training Loop\nepochs = 3\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n    # Reset accuracy at start of epoch\n    train_accuracy.reset_state()\n    test_accuracy.reset_state()\n\n    # Training\n    for batch_inputs, batch_labels in train_dataset:\n        loss = train_step(batch_inputs, batch_labels)\n\n    # Testing\n    for batch_inputs, batch_labels in test_dataset:\n        test_loss = test_step(batch_inputs, batch_labels)\n\n    print(f\"Train Loss: {loss.numpy():.4f}, Train Accuracy: {train_accuracy.result().numpy():.4f}\")\n    print(f\"Test Loss: {test_loss.numpy():.4f}, Test Accuracy: {test_accuracy.result().numpy():.4f}\")\n\n# Save model\nroberta_model.save_pretrained(\"/mnt/data/roberta_sentiment_model\")\ntokenizer.save_pretrained(\"/mnt/data/roberta_tokenizer\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T02:17:15.040410Z","iopub.execute_input":"2025-02-08T02:17:15.040793Z","iopub.status.idle":"2025-02-08T02:27:15.181622Z","shell.execute_reply.started":"2025-02-08T02:17:15.040763Z","shell.execute_reply":"2025-02-08T02:27:15.180658Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/3\nTrain Loss: 0.7854, Train Accuracy: 0.8020\nTest Loss: 0.1628, Test Accuracy: 0.8583\n\nEpoch 2/3\nTrain Loss: 0.3418, Train Accuracy: 0.8676\nTest Loss: 0.1544, Test Accuracy: 0.8596\n\nEpoch 3/3\nTrain Loss: 0.3315, Train Accuracy: 0.9032\nTest Loss: 0.1566, Test Accuracy: 0.8552\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('/mnt/data/roberta_tokenizer/tokenizer_config.json',\n '/mnt/data/roberta_tokenizer/special_tokens_map.json',\n '/mnt/data/roberta_tokenizer/vocab.json',\n '/mnt/data/roberta_tokenizer/merges.txt',\n '/mnt/data/roberta_tokenizer/added_tokens.json',\n '/mnt/data/roberta_tokenizer/tokenizer.json')"},"metadata":{}}],"execution_count":8}]}