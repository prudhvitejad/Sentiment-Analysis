{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport re\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n\n# Load Dataset\nfile_path = \"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\"\ndf = pd.read_csv(file_path)\n\n# Preprocess text\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n    text = re.sub(r\"@\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text.strip()\n\ndf[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n\n# Encode labels\nsentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\ndf[\"label\"] = df[\"airline_sentiment\"].map(sentiment_mapping)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"label\"], test_size=0.2, random_state=42)\n\n# Tokenize text for Word2Vec\nsentences = [text.split() for text in df[\"cleaned_text\"]]\n\n# Train Word2Vec model\nword2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\nvocab_size = len(word2vec_model.wv)\nembedding_dim = 100\n\n# Tokenize text for LSTM/GRU input\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nmax_length = 100\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n\n# Prepare embedding matrix from Word2Vec\nembedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    if index < vocab_size and word in word2vec_model.wv:\n        embedding_matrix[index] = word2vec_model.wv[word]\n\n# Convert labels to categorical\nnum_classes = len(sentiment_mapping)\ny_train_cat = to_categorical(y_train, num_classes=num_classes)\ny_test_cat = to_categorical(y_test, num_classes=num_classes)\n\n# Build LSTM/GRU Model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size + 1, \n                    output_dim=embedding_dim, \n                    weights=[embedding_matrix], \n                    input_length=max_length, \n                    trainable=False))\n\n# Choose either LSTM or GRU by uncommenting one of the following:\n# LSTM Layer\n# model.add(LSTM(units=128, return_sequences=False))\n# GRU Layer\nmodel.add(GRU(units=128, return_sequences=False))\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=num_classes, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n\n# Model Summary\nmodel.summary()\n\n# Train the model\nepochs = 10\nbatch_size = 32\nhistory = model.fit(X_train_pad, y_train_cat, \n                    validation_data=(X_test_pad, y_test_cat), \n                    epochs=epochs, \n                    batch_size=batch_size)\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(X_test_pad, y_test_cat)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Predictions and Classification Report\ny_pred = model.predict(X_test_pad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test_cat, axis=1)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred_classes, target_names=sentiment_mapping.keys()))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:22:18.507800Z","iopub.execute_input":"2025-02-13T13:22:18.508114Z","iopub.status.idle":"2025-02-13T13:22:56.853942Z","shell.execute_reply.started":"2025-02-13T13:22:18.508091Z","shell.execute_reply":"2025-02-13T13:22:56.853054Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │         \u001b[38;5;34m640,400\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">640,400</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m640,400\u001b[0m (2.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">640,400</span> (2.44 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m640,400\u001b[0m (2.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">640,400</span> (2.44 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.6222 - loss: 0.9555 - val_accuracy: 0.6452 - val_loss: 0.9015\nEpoch 2/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6242 - loss: 0.9230 - val_accuracy: 0.6452 - val_loss: 0.8987\nEpoch 3/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6185 - loss: 0.9292 - val_accuracy: 0.6452 - val_loss: 0.8941\nEpoch 4/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6209 - loss: 0.9261 - val_accuracy: 0.6452 - val_loss: 0.8944\nEpoch 5/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6217 - loss: 0.9251 - val_accuracy: 0.6452 - val_loss: 0.8979\nEpoch 6/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6243 - loss: 0.9225 - val_accuracy: 0.6452 - val_loss: 0.8957\nEpoch 7/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6277 - loss: 0.9173 - val_accuracy: 0.6452 - val_loss: 0.8972\nEpoch 8/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6213 - loss: 0.9258 - val_accuracy: 0.6452 - val_loss: 0.8948\nEpoch 9/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6170 - loss: 0.9291 - val_accuracy: 0.6452 - val_loss: 0.8944\nEpoch 10/10\n\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6279 - loss: 0.9165 - val_accuracy: 0.6452 - val_loss: 0.8978\n\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6291 - loss: 0.9153\nTest Accuracy: 64.52%\n\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n              precision    recall  f1-score   support\n\n    negative       0.65      1.00      0.78      1889\n     neutral       0.00      0.00      0.00       580\n    positive       0.00      0.00      0.00       459\n\n    accuracy                           0.65      2928\n   macro avg       0.22      0.33      0.26      2928\nweighted avg       0.42      0.65      0.51      2928\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":2}]}