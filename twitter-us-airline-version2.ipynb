{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:06:54.981619Z","iopub.execute_input":"2025-01-14T15:06:54.981935Z","iopub.status.idle":"2025-01-14T15:06:55.098093Z","shell.execute_reply.started":"2025-01-14T15:06:54.981913Z","shell.execute_reply":"2025-01-14T15:06:55.097315Z"}},"outputs":[{"name":"stdout","text":"twitter-us-airline\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_scheduler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Load and Preprocess Data\ndef clean_text(text):\n    import re\n    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)    # Remove mentions\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n    return text.strip().lower()\n\ndata = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")\ndata[\"text\"] = data[\"text\"].apply(clean_text)\n\n# Handle class imbalance\nlabels = data[\"airline_sentiment\"].map({\"positive\": 2, \"neutral\": 1, \"negative\": 0})\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=[0, 1, 2], y=labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float32).to(\"cuda\")\n\n# Split data\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    data[\"text\"], labels, test_size=0.2, random_state=42\n)\n\n# Step 2: Tokenize Data\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef tokenize_function(texts, labels):\n    tokens = tokenizer(\n        list(texts), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n    )\n    return tokens[\"input_ids\"], tokens[\"attention_mask\"], torch.tensor(labels.values, dtype=torch.long)\n\ntrain_inputs, train_masks, train_labels = tokenize_function(train_texts, train_labels)\nval_inputs, val_masks, val_labels = tokenize_function(val_texts, val_labels)\n\nclass SentimentDataset(Dataset):\n    def __init__(self, inputs, masks, labels):\n        self.inputs = inputs\n        self.masks = masks\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.masks[idx], self.labels[idx]\n\ntrain_dataset = SentimentDataset(train_inputs, train_masks, train_labels)\nval_dataset = SentimentDataset(val_inputs, val_masks, val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Step 3: Model Setup\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\nmodel.to(\"cuda\")\n\n# Step 4: Optimization and Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nepochs = 1\nscheduler = get_scheduler(\n    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs\n)\n\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n\n# Step 5: Training and Validation Loops\ndef train_epoch(model, loader):\n    model.train()\n    total_loss, total_correct = 0, 0\n    for inputs, masks, labels in loader:\n        inputs, masks, labels = inputs.to(\"cuda\"), masks.to(\"cuda\"), labels.to(\"cuda\")\n\n        optimizer.zero_grad()\n        outputs = model(inputs, attention_mask=masks)\n        loss = loss_fn(outputs.logits, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        preds = outputs.logits.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n\n    return total_loss / len(loader), total_correct / len(loader.dataset)\n\ndef validate_epoch(model, loader):\n    model.eval()\n    total_loss, total_correct = 0, 0\n    with torch.no_grad():\n        for inputs, masks, labels in loader:\n            inputs, masks, labels = inputs.to(\"cuda\"), masks.to(\"cuda\"), labels.to(\"cuda\")\n            outputs = model(inputs, attention_mask=masks)\n            loss = loss_fn(outputs.logits, labels)\n            total_loss += loss.item()\n            preds = outputs.logits.argmax(dim=1)\n            total_correct += (preds == labels).sum().item()\n\n    return total_loss / len(loader), total_correct / len(loader.dataset)\n\nfor epoch in range(epochs):\n    train_loss, train_acc = train_epoch(model, train_loader)\n    val_loss, val_acc = validate_epoch(model, val_loader)\n    print(f\"Epoch {epoch + 1}/{epochs}:\")\n    print(f\"  Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n    print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n\n# Step 6: Evaluation\nmodel.eval()\npreds, true_labels = [], []\nwith torch.no_grad():\n    for inputs, masks, labels in val_loader:\n        inputs, masks, labels = inputs.to(\"cuda\"), masks.to(\"cuda\"), labels.to(\"cuda\")\n        outputs = model(inputs, attention_mask=masks)\n        preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n\nprint(classification_report(true_labels, preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:09:33.056950Z","iopub.execute_input":"2025-01-14T15:09:33.057315Z","iopub.status.idle":"2025-01-14T15:12:19.063867Z","shell.execute_reply.started":"2025-01-14T15:09:33.057282Z","shell.execute_reply":"2025-01-14T15:12:19.062925Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1:\n  Train Loss: 0.5773, Train Accuracy: 0.7774\n  Val Loss: 0.4670, Val Accuracy: 0.8286\n              precision    recall  f1-score   support\n\n    Negative       0.95      0.85      0.89      1889\n     Neutral       0.64      0.72      0.68       580\n    Positive       0.70      0.88      0.78       459\n\n    accuracy                           0.83      2928\n   macro avg       0.76      0.82      0.78      2928\nweighted avg       0.85      0.83      0.83      2928\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#version3 code here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nimport numpy as np\nimport random\n\n# Fix random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Step 1: Load and Preprocess Data\ndef clean_text(text):\n    import re\n    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)    # Remove mentions\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n    return text.strip().lower()\n\n# Load dataset\ndataset_path = \"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\"\ndf = pd.read_csv(dataset_path)\n\n# Apply text cleaning\ndf[\"clean_text\"] = df[\"text\"].apply(clean_text)\n\n# Extract cleaned texts and labels\ntexts = df[\"clean_text\"].tolist()\nlabels = df[\"airline_sentiment\"].map({\"negative\": 0, \"neutral\": 1, \"positive\": 2}).tolist()\n\n# Step 2: Oversample Data for Class Balancing\ndef oversample_data(texts, labels):\n    data = list(zip(texts, labels))\n    negative = [x for x in data if x[1] == 0]\n    neutral = [x for x in data if x[1] == 1]\n    positive = [x for x in data if x[1] == 2]\n\n    neutral_upsampled = resample(neutral, replace=True, n_samples=len(negative), random_state=42)\n    positive_upsampled = resample(positive, replace=True, n_samples=len(negative), random_state=42)\n\n    balanced_data = negative + neutral_upsampled + positive_upsampled\n    random.shuffle(balanced_data)\n    return zip(*balanced_data)\n\n# Step 3: Dataset Class\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n# Step 4: Attention Layer\nclass AttentionLayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Linear(hidden_size, 1, bias=False)\n\n    def forward(self, rnn_output):\n        weights = torch.softmax(self.attention(rnn_output), dim=1)\n        weighted_output = torch.sum(weights * rnn_output, dim=1)\n        return weighted_output, weights\n\n# Step 5: Base Model with RoBERTa + RNN + Attention\nclass RoBERTaRNNWithAttention(nn.Module):\n    def __init__(self, model_type=\"lstm\", hidden_size=128, num_classes=3):\n        super(RoBERTaRNNWithAttention, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n        self.rnn_type = model_type.lower()\n        self.hidden_size = hidden_size\n\n        if self.rnn_type == \"lstm\":\n            self.rnn = nn.LSTM(self.roberta.config.hidden_size, hidden_size, batch_first=True, bidirectional=True)\n        elif self.rnn_type == \"bilstm\":\n            self.rnn = nn.LSTM(self.roberta.config.hidden_size, hidden_size, batch_first=True, bidirectional=True)\n        elif self.rnn_type == \"gru\":\n            self.rnn = nn.GRU(self.roberta.config.hidden_size, hidden_size, batch_first=True, bidirectional=True)\n        else:\n            raise ValueError(f\"Unsupported RNN type: {model_type}\")\n\n        self.attention = AttentionLayer(hidden_size * 2)  # Bidirectional doubles the size\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():  # Freeze RoBERTa during training\n            roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = roberta_output.last_hidden_state\n        rnn_output, _ = self.rnn(last_hidden_state)\n        attn_output, _ = self.attention(rnn_output)\n        logits = self.fc(attn_output)\n        return logits\n\n# Step 6: Ensemble Model\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleList(models)\n        self.fc = nn.Linear(len(models) * 3, 3)\n\n    def forward(self, input_ids, attention_mask):\n        logits_list = [model(input_ids, attention_mask) for model in self.models]\n        logits = torch.cat(logits_list, dim=1)  # Concatenate logits\n        return self.fc(logits)\n\n# Step 7: Oversample and Create Dataset\nbalanced_texts, balanced_labels = oversample_data(texts, labels)\n\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\ndataset = SentimentDataset(balanced_texts, balanced_labels, tokenizer)\n\n# Split dataset\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize models\nlstm_model = RoBERTaRNNWithAttention(model_type=\"lstm\").to(device)\nbilstm_model = RoBERTaRNNWithAttention(model_type=\"bilstm\").to(device)\ngru_model = RoBERTaRNNWithAttention(model_type=\"gru\").to(device)\n\nensemble_model = EnsembleModel([lstm_model, bilstm_model, gru_model]).to(device)\n\n# Optimizer and loss\noptimizer = torch.optim.AdamW(ensemble_model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    ensemble_model.train()\n    train_loss, train_acc = 0, 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        logits = ensemble_model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n\n    val_loss, val_acc, val_preds, val_labels = 0, 0, [], []\n    ensemble_model.eval()\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            logits = ensemble_model(input_ids, attention_mask)\n            loss = criterion(logits, labels)\n\n            val_loss += loss.item()\n            val_acc += (logits.argmax(dim=1) == labels).sum().item()\n            val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_acc / len(train_dataset):.4f}\")\n    print(f\"Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_acc / len(val_dataset):.4f}\")\n\n# Classification report\nprint(classification_report(val_labels, val_preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\n# Save model\ntorch.save(ensemble_model.state_dict(), \"ensemble_model.pth\")\ntokenizer.save_pretrained(\"ensemble_model_tokenizer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:44:31.919351Z","iopub.execute_input":"2025-01-14T15:44:31.919680Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6322ac3126f249719a271f298669e1d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046c9e9e1e4c40c0932633bdb18644de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e59f3003b62b4f9d88b209b8f9926e65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b869cc07c7fa4bc8880ecc4e75487ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76081db96e1648f6b6d7cfd32d8c80d8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9869b1f44fc1440fa47f7c9599789f68"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":null}]}