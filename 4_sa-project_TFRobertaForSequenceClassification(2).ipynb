{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModelForSequenceClassification, AutoTokenizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\n\n# Load dataset\nfile_path = \"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\"\ndf = pd.read_csv(file_path)\n\n# Preprocess text\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n    text = re.sub(r\"@\\w+\", \"\", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text.strip()\n\ndf[\"cleaned_text\"] = df[\"text\"].apply(preprocess_text)\n\n# Encode labels\nsentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\ndf[\"label\"] = df[\"airline_sentiment\"].map(sentiment_mapping)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_text\"], df[\"label\"], test_size=0.2, random_state=42)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\n# Tokenization\ndef encode_texts(texts, tokenizer, max_len=128):\n    return tokenizer(list(texts), max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n\nX_train_enc = encode_texts(X_train, tokenizer)\nX_test_enc = encode_texts(X_test, tokenizer)\n\n# Convert labels to NumPy arrays\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train_enc), y_train)).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_enc), y_test)).batch(32).prefetch(tf.data.AUTOTUNE)\n\n# Load RoBERTa model\nroberta_model = TFAutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3, from_pt=True)\n\n# Define optimizer, loss, and metrics\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n\n# Compile the model\nroberta_model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n\n# Train the model using model.fit()\nepochs = 10\nroberta_model.fit(train_dataset, validation_data=test_dataset, epochs=epochs)\n\n# Save model\nroberta_model.save_pretrained(\"/mnt/data/roberta_sentiment_model\")\ntokenizer.save_pretrained(\"/mnt/data/roberta_tokenizer\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T14:20:33.489482Z","iopub.execute_input":"2025-02-12T14:20:33.489833Z","iopub.status.idle":"2025-02-12T14:53:02.323051Z","shell.execute_reply.started":"2025-02-12T14:20:33.489793Z","shell.execute_reply":"2025-02-12T14:53:02.322127Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f3b72709054262a22ac801c38b49e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cb79f2f75249c6a9fc1dfeac9f18c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e3a7f7c79f4f6d9f4db5eda16c4436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c45e8907e54d6587cee1ea6596544b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dc1e1f2f4e2412ba4955513b03d8504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6495d13bd050449b8c5880d1e1cd2e79"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n366/366 [==============================] - 225s 519ms/step - loss: 0.5193 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.3974 - val_sparse_categorical_accuracy: 0.8559\nEpoch 2/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.3476 - sparse_categorical_accuracy: 0.8676 - val_loss: 0.4198 - val_sparse_categorical_accuracy: 0.8528\nEpoch 3/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9068 - val_loss: 0.4568 - val_sparse_categorical_accuracy: 0.8514\nEpoch 4/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.1916 - sparse_categorical_accuracy: 0.9357 - val_loss: 0.5325 - val_sparse_categorical_accuracy: 0.8453\nEpoch 5/10\n366/366 [==============================] - 187s 510ms/step - loss: 0.1520 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.5448 - val_sparse_categorical_accuracy: 0.8531\nEpoch 6/10\n366/366 [==============================] - 186s 510ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9598 - val_loss: 0.5993 - val_sparse_categorical_accuracy: 0.8408\nEpoch 7/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.6411 - val_sparse_categorical_accuracy: 0.8460\nEpoch 8/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.0758 - sparse_categorical_accuracy: 0.9773 - val_loss: 0.6683 - val_sparse_categorical_accuracy: 0.8494\nEpoch 9/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.7396 - val_sparse_categorical_accuracy: 0.8385\nEpoch 10/10\n366/366 [==============================] - 186s 509ms/step - loss: 0.0619 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.7397 - val_sparse_categorical_accuracy: 0.8511\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"('/mnt/data/roberta_tokenizer/tokenizer_config.json',\n '/mnt/data/roberta_tokenizer/special_tokens_map.json',\n '/mnt/data/roberta_tokenizer/vocab.json',\n '/mnt/data/roberta_tokenizer/merges.txt',\n '/mnt/data/roberta_tokenizer/added_tokens.json',\n '/mnt/data/roberta_tokenizer/tokenizer.json')"},"metadata":{}}],"execution_count":1}]}