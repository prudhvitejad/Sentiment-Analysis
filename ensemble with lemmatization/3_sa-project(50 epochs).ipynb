{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10223141,"sourceType":"datasetVersion","datasetId":6319953},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport subprocess\nimport os\n\n# Specify the download directory\nnltk_data_dir = '/kaggle/working/nltk_data/'\n\n# Create the directory if it doesn't exist\nos.makedirs(nltk_data_dir, exist_ok=True)\n\n# Add the directory to NLTK's data path\nnltk.data.path.append(nltk_data_dir)\n\n# Function to download and unzip NLTK resources\ndef download_and_unzip(resource):\n    nltk.download(resource, download_dir=nltk_data_dir)\n    zip_path = os.path.join(nltk_data_dir, 'corpora', f'{resource}.zip')\n    if os.path.exists(zip_path):\n        command = f\"unzip -o {zip_path} -d {os.path.join(nltk_data_dir, 'corpora')}\"\n        subprocess.run(command.split())\n\n# Download and unzip the necessary resources\nresources = ['wordnet', 'averaged_perceptron_tagger', 'punkt']\nfor resource in resources:\n    download_and_unzip(resource)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize stopword set and lemmatizer\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    # Tokenization and Stopword Removal\n    words = text.split()\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=50, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T20:35:24.778997Z","iopub.execute_input":"2025-03-13T20:35:24.779429Z","execution_failed":"2025-03-14T01:30:59.678Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c96d5f8ee6324f3f83d18813e2c9b5c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3cf3ccf72364a22b4ca2c03d745f562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bd9a7eabdc14c4598faafcc5028d681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39b466940be74165a1f22f2c9b7dabb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c7962a0089425bbb4827fad93cbb1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aea7d322587466898654c3321b9b8ef"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n1464/1464 [==============================] - 292s 173ms/step - loss: 0.5834 - accuracy: 0.7646 - val_loss: 0.5314 - val_accuracy: 0.7941\nEpoch 2/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.4487 - accuracy: 0.8227 - val_loss: 0.4802 - val_accuracy: 0.8180\nEpoch 3/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.3620 - accuracy: 0.8601 - val_loss: 0.5319 - val_accuracy: 0.8279\nEpoch 4/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.2709 - accuracy: 0.8999 - val_loss: 0.6436 - val_accuracy: 0.8152\nEpoch 5/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.2049 - accuracy: 0.9239 - val_loss: 0.6017 - val_accuracy: 0.8111\nEpoch 6/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1588 - accuracy: 0.9459 - val_loss: 0.7039 - val_accuracy: 0.8163\nEpoch 7/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1334 - accuracy: 0.9530 - val_loss: 0.7373 - val_accuracy: 0.8183\nEpoch 8/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1074 - accuracy: 0.9630 - val_loss: 0.8210 - val_accuracy: 0.8098\nEpoch 9/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1005 - accuracy: 0.9651 - val_loss: 0.8423 - val_accuracy: 0.8111\nEpoch 10/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0786 - accuracy: 0.9731 - val_loss: 0.8909 - val_accuracy: 0.8132\nEpoch 11/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0766 - accuracy: 0.9747 - val_loss: 0.9648 - val_accuracy: 0.8101\nEpoch 12/50\n1464/1464 [==============================] - 251s 172ms/step - loss: 0.0696 - accuracy: 0.9754 - val_loss: 1.0393 - val_accuracy: 0.8306\nEpoch 13/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0690 - accuracy: 0.9760 - val_loss: 0.9696 - val_accuracy: 0.8166\nEpoch 14/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0651 - accuracy: 0.9765 - val_loss: 0.9438 - val_accuracy: 0.7968\nEpoch 15/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0578 - accuracy: 0.9829 - val_loss: 1.0050 - val_accuracy: 0.8173\nEpoch 16/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0590 - accuracy: 0.9789 - val_loss: 0.9096 - val_accuracy: 0.8070\nEpoch 17/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0532 - accuracy: 0.9819 - val_loss: 1.1117 - val_accuracy: 0.8173\nEpoch 18/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.9053 - val_accuracy: 0.8156\nEpoch 19/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 1.0783 - val_accuracy: 0.8122\nEpoch 20/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0477 - accuracy: 0.9831 - val_loss: 1.2484 - val_accuracy: 0.8139\nEpoch 21/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0501 - accuracy: 0.9827 - val_loss: 1.0732 - val_accuracy: 0.8173\nEpoch 22/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 1.1468 - val_accuracy: 0.8156\nEpoch 23/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0472 - accuracy: 0.9839 - val_loss: 0.9243 - val_accuracy: 0.8190\nEpoch 24/50\n1464/1464 [==============================] - 251s 172ms/step - loss: 0.0423 - accuracy: 0.9856 - val_loss: 1.1126 - val_accuracy: 0.8163\nEpoch 25/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0467 - accuracy: 0.9835 - val_loss: 1.0761 - val_accuracy: 0.8098\nEpoch 26/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0404 - accuracy: 0.9861 - val_loss: 1.0882 - val_accuracy: 0.8087\nEpoch 27/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 1.2445 - val_accuracy: 0.8125\nEpoch 28/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0384 - accuracy: 0.9866 - val_loss: 1.1146 - val_accuracy: 0.8040\nEpoch 29/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0369 - accuracy: 0.9861 - val_loss: 1.1134 - val_accuracy: 0.8207\nEpoch 30/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0380 - accuracy: 0.9871 - val_loss: 1.2191 - val_accuracy: 0.8101\nEpoch 31/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0360 - accuracy: 0.9866 - val_loss: 1.2204 - val_accuracy: 0.8180\nEpoch 32/50\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0378 - accuracy: 0.9875 - val_loss: 1.1274 - val_accuracy: 0.8149\nEpoch 33/50\n1464/1464 [==============================] - 251s 171ms/step - loss: 0.0386 - accuracy: 0.9872 - val_loss: 1.1279 - val_accuracy: 0.8132\nEpoch 34/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0319 - accuracy: 0.9885 - val_loss: 1.2487 - val_accuracy: 0.8197\nEpoch 35/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0301 - accuracy: 0.9901 - val_loss: 1.2705 - val_accuracy: 0.8183\nEpoch 36/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0384 - accuracy: 0.9863 - val_loss: 1.2128 - val_accuracy: 0.8159\nEpoch 37/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0320 - accuracy: 0.9881 - val_loss: 1.1033 - val_accuracy: 0.8105\nEpoch 38/50\n1464/1464 [==============================] - 252s 172ms/step - loss: 0.0372 - accuracy: 0.9861 - val_loss: 1.2573 - val_accuracy: 0.8139\nEpoch 39/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0321 - accuracy: 0.9877 - val_loss: 1.1500 - val_accuracy: 0.8077\nEpoch 40/50\n1464/1464 [==============================] - 253s 173ms/step - loss: 0.0372 - accuracy: 0.9878 - val_loss: 1.2313 - val_accuracy: 0.8152\nEpoch 41/50\n 558/1464 [==========>...................] - ETA: 2:24 - loss: 0.0249 - accuracy: 0.9926","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport subprocess\nimport os\n\n# Specify the download directory\nnltk_data_dir = '/kaggle/working/nltk_data/'\n\n# Create the directory if it doesn't exist\nos.makedirs(nltk_data_dir, exist_ok=True)\n\n# Add the directory to NLTK's data path\nnltk.data.path.append(nltk_data_dir)\n\n# Function to download and unzip NLTK resources\ndef download_and_unzip(resource):\n    nltk.download(resource, download_dir=nltk_data_dir)\n    zip_path = os.path.join(nltk_data_dir, 'corpora', f'{resource}.zip')\n    if os.path.exists(zip_path):\n        command = f\"unzip -o {zip_path} -d {os.path.join(nltk_data_dir, 'corpora')}\"\n        subprocess.run(command.split())\n\n# Download and unzip the necessary resources\nresources = ['wordnet', 'averaged_perceptron_tagger', 'punkt']\nfor resource in resources:\n    download_and_unzip(resource)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize stopword set and lemmatizer\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    # Tokenization and Stopword Removal\n    words = text.split()\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-50k-movies/IMDb_50K_movies/IMDB Dataset.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=50, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T15:54:44.353458Z","iopub.execute_input":"2025-03-14T15:54:44.353752Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9ed5fd380e4153967996c12eade511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0089270795354885902bab6ec28e011d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"444e872a60a144118457b3b44c66f395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf676c11a6af4a8dada1cf3f6173a493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe1dd498c3844ba90009023ff774919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1bb2508fcbe4c548d387acf76bd4202"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n5000/5000 [==============================] - 906s 173ms/step - loss: 0.3207 - accuracy: 0.8632 - val_loss: 0.2856 - val_accuracy: 0.8937\nEpoch 2/50\n5000/5000 [==============================] - 859s 172ms/step - loss: 0.2226 - accuracy: 0.9128 - val_loss: 0.2947 - val_accuracy: 0.8836\nEpoch 3/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.1549 - accuracy: 0.9413 - val_loss: 0.2872 - val_accuracy: 0.8964\nEpoch 4/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.1029 - accuracy: 0.9638 - val_loss: 0.3851 - val_accuracy: 0.8862\nEpoch 5/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0791 - accuracy: 0.9729 - val_loss: 0.4030 - val_accuracy: 0.8983\nEpoch 6/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0644 - accuracy: 0.9783 - val_loss: 0.4319 - val_accuracy: 0.8925\nEpoch 7/50\n5000/5000 [==============================] - 856s 171ms/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 0.3800 - val_accuracy: 0.8999\nEpoch 8/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0447 - accuracy: 0.9849 - val_loss: 0.3939 - val_accuracy: 0.9009\nEpoch 9/50\n5000/5000 [==============================] - 857s 171ms/step - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.3863 - val_accuracy: 0.8950\nEpoch 10/50\n2693/5000 [===============>..............] - ETA: 6:05 - loss: 0.0316 - accuracy: 0.9899","output_type":"stream"}],"execution_count":null}]}