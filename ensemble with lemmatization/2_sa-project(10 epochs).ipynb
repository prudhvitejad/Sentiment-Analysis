{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10223141,"sourceType":"datasetVersion","datasetId":6319953},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport subprocess\nimport os\n\n# Specify the download directory\nnltk_data_dir = '/kaggle/working/nltk_data/'\n\n# Create the directory if it doesn't exist\nos.makedirs(nltk_data_dir, exist_ok=True)\n\n# Add the directory to NLTK's data path\nnltk.data.path.append(nltk_data_dir)\n\n# Function to download and unzip NLTK resources\ndef download_and_unzip(resource):\n    nltk.download(resource, download_dir=nltk_data_dir)\n    zip_path = os.path.join(nltk_data_dir, 'corpora', f'{resource}.zip')\n    if os.path.exists(zip_path):\n        command = f\"unzip -o {zip_path} -d {os.path.join(nltk_data_dir, 'corpora')}\"\n        subprocess.run(command.split())\n\n# Download and unzip the necessary resources\nresources = ['wordnet', 'averaged_perceptron_tagger', 'punkt']\nfor resource in resources:\n    download_and_unzip(resource)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize stopword set and lemmatizer\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    # Tokenization and Stopword Removal\n    words = text.split()\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=10, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T04:39:46.084334Z","iopub.execute_input":"2025-03-13T04:39:46.084647Z","iopub.status.idle":"2025-03-13T05:23:02.864620Z","shell.execute_reply.started":"2025-03-13T04:39:46.084625Z","shell.execute_reply":"2025-03-13T05:23:02.863919Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af24c0675f9a44be83a853b349fed2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6ea8cd840f8485aa1cc1d79134417ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d83e9b73373840dbaa1b776e8a132836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb09287a1764672a2612560c4471a3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"704cea232207424e8b7c2dac08f8d42b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e01a11c00647a88c6baa344d4afa67"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n1464/1464 [==============================] - 292s 173ms/step - loss: 0.5728 - accuracy: 0.7706 - val_loss: 0.4818 - val_accuracy: 0.8183\nEpoch 2/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.4447 - accuracy: 0.8265 - val_loss: 0.4739 - val_accuracy: 0.8176\nEpoch 3/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.3465 - accuracy: 0.8689 - val_loss: 0.4854 - val_accuracy: 0.8217\nEpoch 4/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.2643 - accuracy: 0.9012 - val_loss: 0.5482 - val_accuracy: 0.8125\nEpoch 5/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1984 - accuracy: 0.9280 - val_loss: 0.6281 - val_accuracy: 0.8207\nEpoch 6/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1499 - accuracy: 0.9469 - val_loss: 0.7176 - val_accuracy: 0.8135\nEpoch 7/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1221 - accuracy: 0.9563 - val_loss: 0.7101 - val_accuracy: 0.8163\nEpoch 8/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.1007 - accuracy: 0.9644 - val_loss: 0.8298 - val_accuracy: 0.8115\nEpoch 9/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0861 - accuracy: 0.9712 - val_loss: 0.8904 - val_accuracy: 0.8125\nEpoch 10/10\n1464/1464 [==============================] - 250s 171ms/step - loss: 0.0854 - accuracy: 0.9700 - val_loss: 0.8989 - val_accuracy: 0.8023\n92/92 [==============================] - 21s 181ms/step - loss: 0.8989 - accuracy: 0.8023\nTest Accuracy: 0.8023\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport subprocess\nimport os\n\n# Specify the download directory\nnltk_data_dir = '/kaggle/working/nltk_data/'\n\n# Create the directory if it doesn't exist\nos.makedirs(nltk_data_dir, exist_ok=True)\n\n# Add the directory to NLTK's data path\nnltk.data.path.append(nltk_data_dir)\n\n# Function to download and unzip NLTK resources\ndef download_and_unzip(resource):\n    nltk.download(resource, download_dir=nltk_data_dir)\n    zip_path = os.path.join(nltk_data_dir, 'corpora', f'{resource}.zip')\n    if os.path.exists(zip_path):\n        command = f\"unzip -o {zip_path} -d {os.path.join(nltk_data_dir, 'corpora')}\"\n        subprocess.run(command.split())\n\n# Download and unzip the necessary resources\nresources = ['wordnet', 'averaged_perceptron_tagger', 'punkt']\nfor resource in resources:\n    download_and_unzip(resource)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize stopword set and lemmatizer\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    # Tokenization and Stopword Removal\n    words = text.split()\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-50k-movies/IMDb_50K_movies/IMDB Dataset.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=10, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:40:49.326980Z","iopub.execute_input":"2025-03-13T12:40:49.327255Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a882bdf2b6814499a52e5d78efedfeac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df73fb6562fc43598eac653b270f4b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7247e3bf221b4ee690637b82300d459f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"968781345442452dbe2231e0f1bfd22d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1d0542b2134deda1e582b3ccbbc6e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264062af524e4100a0d2a33fb5da3fff"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.weight', 'lm_head.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n5000/5000 [==============================] - 898s 172ms/step - loss: 0.3221 - accuracy: 0.8629 - val_loss: 0.2527 - val_accuracy: 0.8959\nEpoch 2/10\n5000/5000 [==============================] - 855s 171ms/step - loss: 0.2271 - accuracy: 0.9103 - val_loss: 0.2576 - val_accuracy: 0.8968\nEpoch 3/10\n5000/5000 [==============================] - 860s 172ms/step - loss: 0.1574 - accuracy: 0.9403 - val_loss: 0.2747 - val_accuracy: 0.8972\nEpoch 4/10\n5000/5000 [==============================] - 860s 172ms/step - loss: 0.1051 - accuracy: 0.9625 - val_loss: 0.3771 - val_accuracy: 0.8929\nEpoch 5/10\n5000/5000 [==============================] - 860s 172ms/step - loss: 0.0779 - accuracy: 0.9734 - val_loss: 0.3660 - val_accuracy: 0.8791\nEpoch 6/10\n3959/5000 [======================>.......] - ETA: 2:45 - loss: 0.0609 - accuracy: 0.9798","output_type":"stream"}],"execution_count":null}]}