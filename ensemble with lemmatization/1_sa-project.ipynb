{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10222701,"sourceType":"datasetVersion","datasetId":6319643},{"sourceId":10223141,"sourceType":"datasetVersion","datasetId":6319953},{"sourceId":10585476,"sourceType":"datasetVersion","datasetId":6550943}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport subprocess\nimport os\n\n# Specify the download directory\nnltk_data_dir = '/kaggle/working/nltk_data/'\n\n# Create the directory if it doesn't exist\nos.makedirs(nltk_data_dir, exist_ok=True)\n\n# Add the directory to NLTK's data path\nnltk.data.path.append(nltk_data_dir)\n\n# Function to download and unzip NLTK resources\ndef download_and_unzip(resource):\n    nltk.download(resource, download_dir=nltk_data_dir)\n    zip_path = os.path.join(nltk_data_dir, 'corpora', f'{resource}.zip')\n    if os.path.exists(zip_path):\n        command = f\"unzip -o {zip_path} -d {os.path.join(nltk_data_dir, 'corpora')}\"\n        subprocess.run(command.split())\n\n# Download and unzip the necessary resources\nresources = ['wordnet', 'averaged_perceptron_tagger', 'punkt']\nfor resource in resources:\n    download_and_unzip(resource)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize stopword set and lemmatizer\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    # Tokenization and Stopword Removal\n    words = text.split()\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/twitter-us-airline/Twitter_US_Airline/Tweets.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=3, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T13:37:33.638307Z","iopub.execute_input":"2025-03-12T13:37:33.638691Z","iopub.status.idle":"2025-03-12T13:51:13.768672Z","shell.execute_reply.started":"2025-03-12T13:37:33.638660Z","shell.execute_reply":"2025-03-12T13:51:13.767953Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n1464/1464 [==============================] - 292s 173ms/step - loss: 0.5830 - accuracy: 0.7646 - val_loss: 0.5171 - val_accuracy: 0.7930\nEpoch 2/3\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.4438 - accuracy: 0.8253 - val_loss: 0.4961 - val_accuracy: 0.8200\nEpoch 3/3\n1464/1464 [==============================] - 249s 170ms/step - loss: 0.3410 - accuracy: 0.8699 - val_loss: 0.5461 - val_accuracy: 0.8210\n92/92 [==============================] - 21s 181ms/step - loss: 0.5461 - accuracy: 0.8210\nTest Accuracy: 0.8210\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nimport subprocess\nimport os\n\n# Specify the download directory\nnltk_data_dir = '/kaggle/working/nltk_data/'\n\n# Create the directory if it doesn't exist\nos.makedirs(nltk_data_dir, exist_ok=True)\n\n# Add the directory to NLTK's data path\nnltk.data.path.append(nltk_data_dir)\n\n# Function to download and unzip NLTK resources\ndef download_and_unzip(resource):\n    nltk.download(resource, download_dir=nltk_data_dir)\n    zip_path = os.path.join(nltk_data_dir, 'corpora', f'{resource}.zip')\n    if os.path.exists(zip_path):\n        command = f\"unzip -o {zip_path} -d {os.path.join(nltk_data_dir, 'corpora')}\"\n        subprocess.run(command.split())\n\n# Download and unzip the necessary resources\nresources = ['wordnet', 'averaged_perceptron_tagger', 'punkt']\nfor resource in resources:\n    download_and_unzip(resource)\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize stopword set and lemmatizer\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\n# Load RoBERTa tokenizer and model\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nroberta_model = TFRobertaModel.from_pretrained(model_name)\n\n# Function to clean and preprocess text\ndef clean_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions (@username)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n\n    # Tokenization and Stopword Removal\n    words = text.split()\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n\n    # Lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)\n\n# Function to preprocess text (tokenization)\ndef preprocess_text(texts, max_length=128):\n    cleaned_texts = [clean_text(text) for text in texts]  # Apply cleaning\n    tokens = tokenizer(cleaned_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"tf\")\n    return {'input_ids': tokens['input_ids'], 'attention_mask': tokens['attention_mask']}\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-50k-movies/IMDb_50K_movies/IMDB Dataset.csv\")  # Ensure file exists\n\n# Encode sentiment labels (Negative=0, Neutral=1, Positive=2)\nlabel_encoder = LabelEncoder()\ndf['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n\n# Split data into train and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n\n# Tokenize and preprocess text data\ntrain_inputs = preprocess_text(list(train_texts))\ntest_inputs = preprocess_text(list(test_texts))\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\n# Define Attention Layer\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W = tf.keras.layers.Dense(units)\n        self.b = tf.keras.layers.Dense(units)\n        self.u = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        score = tf.nn.tanh(self.W(inputs) + self.b(inputs))\n        attention_weights = tf.nn.softmax(self.u(score), axis=1)\n        context_vector = attention_weights * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector\n\n# Define Ensemble Model: RoBERTa + LSTM + BiLSTM + GRU + Attention\nclass RobertaEnsembleModel(tf.keras.Model):\n    def __init__(self, roberta_model, lstm_units=128, num_classes=3):\n        super(RobertaEnsembleModel, self).__init__()\n        self.roberta = roberta_model\n        \n        # Recurrent layers\n        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True)\n        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))\n        self.gru = tf.keras.layers.GRU(lstm_units, return_sequences=True)\n        \n        # Attention layers\n        self.lstm_attention = Attention(lstm_units)\n        self.bilstm_attention = Attention(lstm_units * 2)  # Bidirectional doubles the units\n        self.gru_attention = Attention(lstm_units)\n\n        # Fully connected layers\n        self.concat = tf.keras.layers.Concatenate()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        roberta_outputs = self.roberta(inputs)[0]\n        lstm_out = self.lstm(roberta_outputs)\n        bilstm_out = self.bilstm(roberta_outputs)\n        gru_out = self.gru(roberta_outputs)\n\n        # Apply Attention to each recurrent layer\n        lstm_attn = self.lstm_attention(lstm_out)\n        bilstm_attn = self.bilstm_attention(bilstm_out)\n        gru_attn = self.gru_attention(gru_out)\n\n        # Merge outputs\n        merged = self.concat([lstm_attn, bilstm_attn, gru_attn])\n        dropout_out = self.dropout(merged)\n        return self.dense(dropout_out)\n\n# Initialize Model\nnum_classes = len(label_encoder.classes_)  # 3 classes (Negative, Neutral, Positive)\nmodel = RobertaEnsembleModel(roberta_model, lstm_units=128, num_classes=num_classes)\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\n# Train Model\nmodel.fit(train_inputs, train_labels, epochs=3, batch_size=8, validation_data=(test_inputs, test_labels))\n\n# Evaluate Model\nloss, accuracy = model.evaluate(test_inputs, test_labels)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T13:55:49.810480Z","iopub.execute_input":"2025-03-12T13:55:49.810933Z","iopub.status.idle":"2025-03-12T14:41:23.224425Z","shell.execute_reply.started":"2025-03-12T13:55:49.810892Z","shell.execute_reply":"2025-03-12T14:41:23.223707Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data/...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'roberta.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n5000/5000 [==============================] - 887s 171ms/step - loss: 0.3245 - accuracy: 0.8628 - val_loss: 0.2577 - val_accuracy: 0.8963\nEpoch 2/3\n5000/5000 [==============================] - 848s 170ms/step - loss: 0.2197 - accuracy: 0.9122 - val_loss: 0.2422 - val_accuracy: 0.9038\nEpoch 3/3\n5000/5000 [==============================] - 847s 169ms/step - loss: 0.1562 - accuracy: 0.9410 - val_loss: 0.2731 - val_accuracy: 0.9038\n313/313 [==============================] - 62s 182ms/step - loss: 0.2731 - accuracy: 0.9038\nTest Accuracy: 0.9038\n","output_type":"stream"}],"execution_count":5}]}